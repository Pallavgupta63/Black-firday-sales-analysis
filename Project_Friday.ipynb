{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C:\\\\Users\\\\Sanjay Gupta\\\\Desktop\\\\ext\\\\train.csv\")\n",
    "test = pd.read_csv(\"C:\\\\Users\\\\Sanjay Gupta\\\\Desktop\\\\ext\\\\test.csv\")\n",
    "submission = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Documents\\\\ML\\\\Project\\\\Sub.csv\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idsUnique = len(set(dataset.User_ID))\n",
    "idsTotal = dataset.shape[0]\n",
    "idsDupli = idsTotal - idsUnique\n",
    "print(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "sns.distplot(dataset.Purchase, bins = 25)\n",
    "plt.xlabel(\"Amount spent in Purchase\")\n",
    "plt.ylabel(\"Number of Buyers\")\n",
    "plt.title(\"Purchase amount Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Skew is:\", dataset.Purchase.skew())\n",
    "print(\"Kurtosis: %f\" % dataset.Purchase.kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check which rows are numbers\n",
    "numeric_features = dataset.select_dtypes(include=[np.number])\n",
    "numeric_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(dataset.Occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(dataset.Marital_Status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(dataset.Product_Category_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(dataset.Product_Category_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(dataset.Product_Category_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To find the dependencies of Purchase of various attributes\n",
    "corr = numeric_features.corr()\n",
    "\n",
    "print (corr['Purchase'].sort_values(ascending=False)[:10],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of columns with high correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "f, ax = plt.subplots(figsize=(14, 7))\n",
    "sns.heatmap(corr, vmax=.8,annot_kws={'size': 14}, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(dataset.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(dataset.Stay_In_Current_City_Years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(dataset.City_Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Occupation_pivot = \\\n",
    "dataset.pivot_table(index='Occupation', values=\"Purchase\", aggfunc=np.mean)\n",
    "\n",
    "Occupation_pivot.plot(kind='bar', color='darkorange',figsize=(13,8))\n",
    "plt.xlabel(\"Occupation\")\n",
    "plt.ylabel(\"Purchase\")\n",
    "plt.title(\"Occupation vs Purchase\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Product_Category_1_pivot=\\\n",
    "dataset.pivot_table(index='Product_Category_1', values=\"Purchase\", aggfunc=np.mean)\n",
    "\n",
    "Product_Category_1_pivot.plot(kind='bar', color='darkorange',figsize=(11,8))\n",
    "plt.xlabel(\"Product_1\")\n",
    "plt.ylabel(\"Purchase\")\n",
    "plt.title(\"Product_1 vs Purchase\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roduct_Category_2_pivot=\\\n",
    "dataset.pivot_table(index='Product_Category_2', values=\"Purchase\")\n",
    "\n",
    "roduct_Category_2_pivot.plot(kind='bar', color='darkgreen',figsize=(11,8))\n",
    "plt.xlabel(\"Product_2\")\n",
    "plt.ylabel(\"Purchase\")\n",
    "plt.title(\"Product_2 vs Purchase\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age1=\\\n",
    "dataset.pivot_table(index='Age', values=\"Purchase\", aggfunc=np.mean)\n",
    "Age1.plot(kind='bar', color='darkgreen',figsize=(10,8))\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Purchase\")\n",
    "plt.title(\"Age vs Purchase\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Occupation1 = \\\n",
    "dataset.pivot_table(index='Marital_Status', values=\"Purchase\", aggfunc=np.mean)\n",
    "\n",
    "Occupation1.plot(kind='bar', color='darkgreen',figsize=(10,8))\n",
    "plt.xlabel(\"Marital_Status\")\n",
    "plt.ylabel(\"Purchase\")\n",
    "plt.title(\"Marital_Status vs Purchase\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "City1 = \\\n",
    "dataset.pivot_table(index='City_Category', values=\"Purchase\", aggfunc=np.mean)\n",
    "\n",
    "City1.plot(kind='bar', color='darkgreen',figsize=(10,8))\n",
    "plt.xlabel(\"City_Category\")\n",
    "plt.ylabel(\"Purchase\")\n",
    "plt.title(\"City_Category vs Purchase\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['User_ID'] = dataset['User_ID'] - 1000000\n",
    "test['User_ID'] = test['User_ID'] - 1000000\n",
    "\n",
    "enc = LabelEncoder()\n",
    "dataset['User_ID'] = enc.fit_transform(dataset['User_ID'])\n",
    "test['User_ID'] = enc.transform(test['User_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Product_ID'] = dataset['Product_ID'].str.replace('P00', '')\n",
    "test['Product_ID'] = test['Product_ID'].str.replace('P00', '')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dataset['Product_ID'] = scaler.fit_transform(dataset['Product_ID'].values.reshape(-1, 1))\n",
    "test['Product_ID'] = scaler.transform(test['Product_ID'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating sets of train and test\n",
    "dataset['source']='train'\n",
    "test['source']='test'\n",
    "\n",
    "data = pd.concat([dataset,test], ignore_index = True, sort = False)\n",
    "\n",
    "print(dataset.shape, test.shape, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()/data.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Product_Category_2\"]=\\\n",
    "data[\"Product_Category_2\"].fillna(-1.0).astype(\"float\")\n",
    "data.Product_Category_2.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Product_Category_3\"]=\\\n",
    "data[\"Product_Category_3\"].fillna(-1.0).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.Product_Category_3.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = data.index[(data.Product_Category_1.isin([19,20])) & (data.source == \"dataset\")]\n",
    "data = data.drop(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Apply function len(unique()) to every data variable\n",
    "data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = data.select_dtypes(include=['object']).columns.drop([\"source\"])\n",
    "#Print frequency of categories\n",
    "for col in category_cols:\n",
    " #Number of times each value appears in the column\n",
    " frequency = data[col].value_counts()\n",
    " print(\"\\nThis is the frequency distribution for \" + col + \":\")\n",
    " print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['Gender'],ages = pd.factorize(data['Gender'])\n",
    "print(ages)\n",
    "print(data['Gender'].unique())\n",
    "data[\"Gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'],ages = pd.factorize(data['Age'])\n",
    "print(ages)\n",
    "print(data['Age'].unique())\n",
    "data[\"Age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Stay_In_Current_City_Years'],scc = pd.factorize(data['Stay_In_Current_City_Years'])\n",
    "print(scc)\n",
    "print(data['Stay_In_Current_City_Years'].unique())\n",
    "data['Stay_In_Current_City_Years'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['City_Category'],cc = pd.factorize(data['City_Category'])\n",
    "print(cc)\n",
    "print(data['City_Category'].unique())\n",
    "data['City_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = ['Gender', 'City_Category']\n",
    "num_col = ['Age', 'Occupation', 'Stay_In_Current_City_Years', 'Product_Category_1', \n",
    "           'Product_Category_2', 'Product_Category_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "data1=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_col:\n",
    "    data1[col] = encoder.fit_transform(data1[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "for col in num_col:\n",
    "    data1[col] = scaler.fit(data1[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountVar(compute_df, count_df, var_name):\n",
    "    grouped_df = count_df.groupby(var_name)\n",
    "    count_dict = {}\n",
    "    for name, group in grouped_df:\n",
    "        count_dict[name] = group.shape[0]\n",
    "    count_list = []\n",
    "    for index, row in compute_df.iterrows():\n",
    "        name = row[var_name]\n",
    "        count_list.append(count_dict.get(name, 0))\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Age_Count\"] =getCountVar(data, data, \"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Occupation_Count\"] =getCountVar(data, data, \"Occupation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Product_Category_1_Count\"] =getCountVar(data, data,\"Product_Category_1\")\n",
    "# data[\"Product_Category_2_Count\"] =getCountVar(data, data,\"Product_Category_2\")\n",
    "# data[\"Product_Category_3_Count\"] =getCountVar(data, data,\"Product_Category_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Product_ID_Count\"] =getCountVar(data, data, \"Product_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide into test and train:\n",
    "train = data.loc[data['source']==\"train\"]\n",
    "test = data.loc[data['source']==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Drop unnecessary columns:\n",
    "test.drop(['source'],axis=1,inplace=True)\n",
    "train.drop(['source'],axis=1,inplace=True)\n",
    "\n",
    "#Export files as modified versions:\n",
    "train.to_csv(\"C:\\\\Users\\\\HP\\\\Documents\\\\ML\\\\Project\\\\train_modified.csv\",index=False)\n",
    "test.to_csv(\"C:\\\\Users\\\\HP\\\\Documents\\\\ML\\\\Project\\\\test_modified.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id_res = data.groupby([\"Product_ID\"])[\"Purchase\"].mean()\n",
    "avg_cost = data[\"Purchase\"].mean()\n",
    "# If i find a product id for which i dont have an avg pricing i will use global vg pricing.\n",
    "product_id_res_map = {}\n",
    "# created a map with product id to avg price map\n",
    "val = product_id_res.iteritems()\n",
    "for key, value in val:\n",
    "    p_id = str(key)\n",
    "    product_id_res_map[p_id] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_purchase_mean(product_id, product_category=None, key=None):\n",
    "    key_pair = str(product_id)\n",
    "    key_pair_pid = str(product_id) + str(product_category)\n",
    "    if key == \"1\":\n",
    "        if key_pair_pid in product_category_1_res:\n",
    "            return product_category_1_res[key_pair_pid]\n",
    "    elif key == \"2\":\n",
    "        if key_pair_pid in product_category_2_res:\n",
    "            return product_category_2_res[key_pair_pid]\n",
    "    elif key == \"3\":\n",
    "        if key_pair_pid in product_category_3_res:\n",
    "            return product_category_3_res[key_pair_pid]\n",
    "    if key_pair in product_id:\n",
    "         return product_id[key_pair]\n",
    "    return avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_purchase_mean(data.Product_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding models to predict purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Documents\\\\ML\\\\Project\\\\train_modified.csv\")\n",
    "test_df = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Documents\\\\ML\\\\Project\\\\test_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Define target and ID columns:\n",
    "# target = 'Item_Outlet_Sales'\n",
    "# IDcol = ['Item_Identifier','Outlet_Identifier']\n",
    "\n",
    "#Define target and ID columns:\n",
    "target = 'Purchase'\n",
    "IDcol = ['User_ID','Product_ID']\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commonfit(alg, dtrain, dtest, predictors, target, IDcol, filename):\n",
    "    #Fitting the algorithm\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "        \n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "\n",
    "    cv_score = cross_val_score(alg, dtrain[predictors],(dtrain[target]) , cv=20, scoring='neg_mean_squared_error')\n",
    "    cv_score = np.sqrt(np.abs(cv_score))\n",
    "    \n",
    "    print(\"\\nModel Report\")\n",
    "    #The value to check is RMSE(parameter)\n",
    "    print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((dtrain[target]).values, dtrain_predictions)))\n",
    "    print(\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "    \n",
    "    dtest[target] = alg.predict(dtest[predictors])\n",
    "    \n",
    "    IDcol.append(target)\n",
    "    submission = pd.DataFrame({ x: dtest[x] for x in IDcol})\n",
    "    submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "LR = LinearRegression(normalize=True)\n",
    "\n",
    "predictors = train_df.columns.drop(['Purchase','Product_ID','User_ID'])\n",
    "commonfit(LR, train_df, test_df, predictors, target, IDcol, 'LR1.csv')\n",
    "\n",
    "coef1 = pd.Series(LR.coef_, predictors).sort_values()\n",
    "coef1.plot(kind='bar', title='Model Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "RR1 = Ridge(alpha=0.05,normalize=True)\n",
    "commonfit(RR1, train_df, test_df, predictors, target, IDcol, 'RR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef1 = pd.Series(RR1.coef_, predictors).sort_values()\n",
    "coef1.plot(kind='bar', title='Model Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "RR1 = Ridge(alpha=3,normalize=True)\n",
    "commonfit(RR1, train_df, test_df, predictors, target, IDcol, 'RR1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef1 = pd.Series(RR1.coef_, predictors).sort_values()\n",
    "coef1.plot(kind='bar', title='Model Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "RR3 = Ridge(alpha=5,normalize=True)\n",
    "commonfit(RR3, train_df, test_df, predictors, target, IDcol, 'RR3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef1 = pd.Series(RR3.coef_, predictors).sort_values()\n",
    "coef1.plot(kind='bar', title='Model Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DT = DecisionTreeRegressor(max_depth=15, min_samples_leaf=200)\n",
    "commonfit(DT, train_df, test_df, predictors, target, IDcol, 'DT.csv')\n",
    "importances = DT.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(train_df[predictors].shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_df[predictors]\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],color=\"y\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = DecisionTreeRegressor(max_depth=9, min_samples_leaf=100)\n",
    "commonfit(RF, train_df, test_df, predictors, target, IDcol,'RF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = RF.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(train_df[predictors].shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_df[predictors]\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],color=\"y\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(train_df[predictors], train_df[target], early_stopping_rounds=5, eval_set=[(test_df[predictors], test_df[target])], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_predictions = my_model.predict(train_df[predictors])\n",
    "\n",
    "predictions = my_model.predict(test_df[predictors])\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_df[target])))\n",
    "print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train_df[target]).values, train_df_predictions)))\n",
    "\n",
    "IDcol.append(target)\n",
    "submission = pd.DataFrame({ x: test_df[x] for x in IDcol})\n",
    "submission.to_csv(\"XGBoost.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = my_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature order:\")\n",
    "for f in range(train_df[predictors].shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_reg = XGBRegressor(learning_rate=1.0, max_depth=6, min_child_weight=40, seed=0)\n",
    "\n",
    "# xgb_reg.fit(train, y_train)\n",
    "# y_pred = xgb_reg.predict(X_val)\n",
    "# rmse = np.sqrt(mean_squared_error(y_pred, y_val))\n",
    "\n",
    "# print xgb_reg, rmse\n",
    "\n",
    "# xgb_reg.fit(X, y)\n",
    "# predict = xgb_reg.predict(X_test)\n",
    "\n",
    "# submission['Purchase'] = predict\n",
    "# submission.to_csv('Sample_Submission_Tm9Lura.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More evenly distributed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_df[predictors]\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],color=\"y\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RF11=RandomForestRegressor(n_estimators=10, max_depth=3)\n",
    "RF21=RandomForestRegressor(n_estimators=10, max_depth=10)\n",
    "RF31=RandomForestRegressor(n_estimators=10, max_depth=20)\n",
    "RF41=RandomForestRegressor(n_estimators=10, max_depth=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF5=RandomForestRegressor(n_estimators=50, max_depth=3)\n",
    "# RF6=RandomForestClassifier(n_estimators=50, max_depth=10)\n",
    "# RF7=RandomForestClassifier(n_estimators=50, max_depth=20)\n",
    "# RF8=RandomForestClassifier(n_estimators=50, max_depth=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF9=RandomForestRegressor(n_estimators=100, max_depth=3)\n",
    "# RF10=RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "# RF11=RandomForestClassifier(n_estimators=100, max_depth=20)\n",
    "# RF12=RandomForestClassifier(n_estimators=100, max_depth=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF13=RandomForestRegressor(n_estimators=200, max_depth=3)\n",
    "# RF14=RandomForestClassifier(n_estimators=200, max_depth=10)\n",
    "# RF15=RandomForestClassifier(n_estimators=200, max_depth=20)\n",
    "# RF16=RandomForestClassifier(n_estimators=200, max_depth=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RF11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonfit(RF11, train_df, test_df, predictors, target, IDcol, 'RF11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF21.fit(train_df[predictors], train_df[target])\n",
    "train_df_predictions = RF21.predict(train_df[predictors])\n",
    "predictions = RF21.predict(test_df[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_df[target])))\n",
    "print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train_df[target]).values, train_df_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF31.fit(train_df[predictors], train_df[target])\n",
    "train_df_predictions = RF31.predict(train_df[predictors])\n",
    "predictions = RF31.predict(test_df[predictors])\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_df[target])))\n",
    "print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train_df[target]).values, train_df_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = RF31.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature order:\")\n",
    "for f in range(train_df[predictors].shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_df[predictors]\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],color=\"y\", align=\"center\")\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF41.fit(train_df[predictors], train_df[target])\n",
    "train_df_predictions1 = RF41.predict(train_df[predictors])\n",
    "predictions = RF41.predict(test_df[predictors])\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_df[target])))\n",
    "print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train_df[target]).values, train_df_predictions1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = RF41.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature order:\")\n",
    "for f in range(train_df[predictors].shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_df[predictors]\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],color=\"y\", align=\"center\")\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF5.fit(train_df[predictors], train_df[target])\n",
    "train_df_predictions = RF5.predict(train_df[predictors])\n",
    "predictions = RF5.predict(test_df[predictors])\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_df[target])))\n",
    "print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train_df[target]).values, train_df_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF9.fit(train_df[predictors], train_df[target])\n",
    "train_df_predictions = RF9.predict(train_df[predictors])\n",
    "predictions = RF9.predict(test_df[predictors])\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_df[target])))\n",
    "print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train_df[target]).values, train_df_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# ET = ExtraTreesRegressor(n_estimators=1450, max_depth=8,min_samples_split=10, min_samples_leaf=10, oob_score=True, n_jobs=6, random_state=123, verbose=1, bootstrap=True)\n",
    "# ET.fit(train_df[predictors], train_df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_predictions = ET.predict(train_df[predictors])\n",
    "predictions = ET.predict(test_df[predictors])\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_df[target])))\n",
    "print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train_df[target]).values, train_df_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df_predictions = ET.predict(train_df[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon as exp\n",
    "from scipy.stats import randint as ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = exp(scale=100)\n",
    "max_depth = ri(1, 40)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF1=RandomForestRegressor(n_estimators=20, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF1.fit(train_df[predictors], train_df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_predictions = RF1.predict(train_df[predictors])\n",
    "predictions = RF1.predict(test_df[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_df[target])))\n",
    "print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((train_df[target]).values, train_df_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Based Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install skope-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_curve\n",
    "# from matplotlib import pyplot as plt\n",
    "# from skrules import SkopeRules\n",
    "\n",
    "# clf = SkopeRules(max_depth_duplication=None,\n",
    "#                  n_estimators=30,\n",
    "#                  precision_min=0.2,\n",
    "#                  recall_min=0.01,\n",
    "#                  feature_names=.feature_names)\n",
    "\n",
    "# X, y = train_df[predictors], train_df[target] > 25\n",
    "# X_train, y_train = X[:len(y)//2], y[:len(y)//2]\n",
    "# X_test, y_test = X[len(y)//2:], y[len(y)//2:]\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_score = clf.score_top_rules(X_test) # Get a risk score for each test example\n",
    "# precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "# plt.plot(recall, precision)\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision Recall curve')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = train_df[(train_df['Product_Category_1'] == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "newtrain = train_df.applymap(encode_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtrain=newtrain.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors1 = train_df.columns.drop(['Product_ID','User_ID','Marital_Status','Stay_In_Current_City_Years'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(newtrain[predictors1], min_support=0.07, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rules[ (rules['lift'] > 1.0) &\n",
    "       (rules['confidence'] > 0.73)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtrain1 = basket.applymap(encode_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "newtrain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(newtrain1[predictors1], min_support=0.07, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[ (rules['lift'] > 1.02) &\n",
    "       (rules['confidence'] > 0.966)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_df[predictors].loc[:2000,]\n",
    "y=train_df[target].loc[:2000,]\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import export_graphviz\n",
    "# import graphviz\n",
    "# graphviz.Source(export_graphviz(clf, out_file = None, feature_names = X.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m  pip install git+git://github.com/christophM/rulefit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m  pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rulefit\n",
    "# from rulefit import RuleFit\n",
    "\n",
    "\n",
    "# y = train_df[target]\n",
    "# X = train_df[predictors]\n",
    "# features = X.columns\n",
    "# X = X.as_matrix()\n",
    "\n",
    "# rf = RuleFit()\n",
    "# rf.fit(X, y, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(clf.decision_path(X).toarray()).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X.reset_index(drop=True),pd.DataFrame(clf.decision_path(X).toarray())],1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree3 = DecisionTreeRegressor(max_depth = 4)\n",
    "commonfit(dTree3, train_df, test_df, predictors, target, IDcol, 'DT.csv')\n",
    "\n",
    "Xrules = pd.concat([X.reset_index(drop=True),pd.DataFrame(dTree3.decision_path(X).toarray()).iloc[:,1:]],1)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(Xrules, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_node(tree_, current_node, search_node, features):\n",
    "    \n",
    "    child_left = tree_.children_left[current_node]\n",
    "    child_right = tree_.children_right[current_node]\n",
    "\n",
    "    split_feature = str(features[tree_.feature[current_node]])\n",
    "    split_value = str(tree_.threshold[current_node])\n",
    "\n",
    "\n",
    "    if child_left != -1:\n",
    "        if child_left != search_node:\n",
    "            left_one = find_node(tree_, child_left, search_node, features)\n",
    "        else:\n",
    "            return(str(split_feature)+\" <= \"+str(split_value))\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "    if child_right != -1:\n",
    "        if child_right != search_node:\n",
    "            right_one = find_node(tree_, child_right, search_node, features)\n",
    "        else:\n",
    "            return(str(split_feature)+\" > \"+str(split_value))\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    if len(left_one)>0:\n",
    "        return(str(split_feature)+\" <= \"+str(split_value)+\", \"+left_one)\n",
    "    elif len(right_one)>0:\n",
    "        return(str(split_feature)+\" > \"+str(split_value)+\",\"+right_one)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_node(tree_ = clf.tree_, current_node = 0, search_node = 13, features = X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[(dataset['Purchase'] >= 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDecisionPath = pd.DataFrame(clf.decision_path(X).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDecisionPath.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree3 = DecisionTreeRegressor(max_depth = 4)\n",
    "commonfit(dTree3, train_df, test_df, predictors, target, IDcol, 'DT.csv')\n",
    "\n",
    "Xrules = pd.concat([X.reset_index(drop=True),pd.DataFrame(dTree3.decision_path(X).toarray()).iloc[:,1:]],1)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(Xrules, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors3=['Product_Category_1','Product_Category_2','Product_Category_3']\n",
    "store_data=train_df[predictors3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data= store_data.applymap(encode_units)\n",
    "store_data=store_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(store_data, min_support=0.07, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree3 = DecisionTreeRegressor(max_depth = 6)\n",
    "commonfit(dTree3, train_df, test_df, predictors, target, IDcol, 'DT.csv')\n",
    "\n",
    "Xrules = pd.concat([X.reset_index(drop=True),pd.DataFrame(dTree3.decision_path(X).toarray()).iloc[:,1:]],1)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "tree_model = DecisionTreeRegressor()\n",
    "tree_model.fit(Xrules, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
